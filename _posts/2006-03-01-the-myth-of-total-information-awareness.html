---
layout: post
title: The Myth of Total Information Awareness
categories: []
tags: []
status: publish
type: post
published: true
meta: {}
---
<p>Lost in the last few weeks in the incessant coverage over Dick Cheney&#8217;s decision to <a href="http://www.theonion.com/content/node/45572">hunt the ultimate prey</a> were some interesting revelations about the <span class="caps">NSA</span>&#8217;s new mechanisms for spying on electronic communications. The Christian Science Monitor broke this story first, report on <a href="http://www.csmonitor.com/2006/0209/p01s02-uspo.html?s=hns">a system named <span class="caps">ADVISE</span> that would spider blogs, wishlists and other relics of online presence to build up dossiers on people</a>. Some people thought that <span class="caps">ADVISE</span> was <a href="http://battellemedia.com/archives/002342.php">simply the rejected Total Information Awareness (TIA) program in new clothing</a>, but Newsweek in a <a href="http://www.msnbc.msn.com/id/11238800/site/newsweek/print/1/displaymode/1098/">truly excellent work of reporting</a> broke the information that <a href="http://thinkprogress.org/2006/02/11/tia-lives/">the core of <span class="caps">TIA</span> was actually renamed to a project called Topsail</a> and <span class="caps">ADVISE</span> was something else. <span class="caps">TIA</span>&#8217;s core motivation was to simply scan communications for &#8220;suspicious activities&#8221; and then notify human analysts of potential problems. <span class="caps">ADVISE</span> seems to have a much grander scope of building up dossiers of people&#8217;s interests and intents to identify &#8220;suspicious people&#8221; instead. Yikes.</p>


	<p>In some sense, this is nothing new. Way back in 1993, the <span class="caps">NSA</span> made waves by trying to pass through a mandatory encryption standard called <a href="http://www.epic.org/crypto/clipper/">the Clipper chip</a> that would enable the government to decrypt any encrypted communications and the <a href="http://fly.hiwaay.net/~pspoole/echres.html">Echelon</a> project has been steadily accumulating intercepted electronic communications under the <span class="caps">NSA</span>&#8217;s purview. But the <span class="caps">NSA</span> has always had issues analyzing the volume of messages they grab and very, very little of the data they retrieve ever makes its way in front of an analyst. Making light of this, some Emacs developer even added a feature <strong>M-x spook</strong> that would spit out a series of suspicious words suitable for activists to add to their email and overwhelm the already overstrained capabilities of the government like so:</p>


	<p><em>top secret <span class="caps">SAFE</span> terrorist <span class="caps">ANZUS </span>New World Order enforcers radar <span class="caps">TELINT </span>Serbian advisors <span class="caps">FIPS140 INSCOM</span> government <span class="caps">CNCIS</span> secure</em></p>


	<p>But here we are again, with the government claiming a need to spy on us and the media leading a fight against it. If it continues like it has though, we fighting this are destined to lose. The problem is that most of the ensuing discussion of the government&#8217;s data mining operations have been like those for the wire tapping scandal; criticism is focused on the political and ethical problems of the systems and <a href="http://mediamatters.org/items/200512240002">lies are exposed</a>, but the underlying technical problems are glossed over by tech-averse journalists. Indeed, most discussions on the legality of wire tapping implicitly assume that the technology is completely effective but should be avoided strictly out of moral concerns. To see what I mean, recall how the debate over torture that started last year played out in newspapers and TV shows. The &#8220;anti-torture&#8221; side would start the argument by positing the moral horror of torture. The &#8220;pro-torture&#8221; side would almost always then counter with a hypothetical situation of capturing a mastermind who we are unquestionably certain knows about a master plot to destroy an American city and from whom torture is the <em>only</em> way to get such information. The torture advocate thus sidesteps the moral horrors of the situation by claiming there is no viable alternative. Of course, <a href="http://www.washingtonpost.com/wp-dyn/articles/A2302-2005Jan11.html">such situations never happen and torture rarely ever yields true information</a>, but that&#8217;s besides the point. The argument is thus glibly reduced to &#8220;idealists vs. pragmatists&#8221;, and in these times the pragmatists always win the debate for public opinion.</p>


	<p>This process will likely happen again with the debate over data mining. <a href="http://www.msnbc.msn.com/id/11238800/site/newsweek/print/1/displaymode/1098/">That Newsweek article</a> does an exemplary job of exploring the technical issues, but they&#8217;re an exception to the rule. The pragmatists will advance the argument that nobody really like spying on Americans, but it&#8217;s the only way to catch the bad guys. And this is what makes me really upset. It&#8217;s bad enough that data mining is likely illegal and invasive, but even more galling that the system most likely will never work in the first place.</p>


	<p><em>eavesdropping emc <span class="caps">ARPA HAMASMOIS </span>Aldergrove <span class="caps">AGT</span>. AMME Freeh White House jihad csystems <span class="caps">MIT</span>-LL 22nd <span class="caps">SAS NWO</span> pink noise mania</em></p>


	<p>So, what are the technical problems that make such a system unfeasible? For starters, this isn&#8217;t actually data mining. I used to work at a data mining software developer (<a href="http://www.dimins.com/">Dimensional Insight</a>) and the goal of those products was to organize complicated data into easily traversable ways for analysts to drill down for connections. In a typical case, you might want to look over your sales data for the last year to see how products sold in particular parts of the country, which sales divisions did best, or similar queries. The process is human-driven and its sole purpose is to represent complex multi-dimensional data (ie, price, product, sales person, city, region, state, time, correlation to other product sales, etc.) in an easily viewable and usable manner to drill down through the data for connections. In addition, data mining involves looking backwards from the present to gain insight into past purchasing patterns to drive future sales (the classic success story is <a href="http://www.dmreview.com/article_sub.cfm?articleId=1006133">a supermarket finding a correlation between diaper and beer sales</a>).</p>


	<p>Instead, the <span class="caps">NSA</span> largely seems to be interested in predicting novel future behavior and retrieving warnings when suspicious activities occur. This is actually more in line with Artificial Intelligence research on classifiers. Essentially what the <span class="caps">NSA</span> seems to be striving for is some sort of theoretical Threat Box which can be fed a steady stream of events and spit out a warning for human analysts to follow up in certain cases. Whether the backend classifier is a neural net, support vector machine, or other sort of technology, the process of training classifiers usually includes the same parts. First, a <strong>training set</strong> needs to be assembled out of a mixture of scored positive and negative events. So, if you were creating a &#8220;terrorist email classifier&#8221;, the positive events might be emails from Osama Bin Ladin, the negative would be emails from your Aunt Sue (there are usually many more negative than positive selections). When the testing is done, a similarly pre-classified <strong>testing set</strong> is used to evaluate how good the classifier actually is. The goal of a well-trained set is to make the correct correlations between certain inputs (so that the presence of &#8220;bomb&#8221; and &#8220;white&#8221; and &#8220;house&#8221; triggers an alarm), but a constant risk with such systems is the danger of erroneously assuming certain correlations are meaningful (eg, all of Mohammed Atta&#8217;s emails included the word &#8220;the&#8221;, so the system concludes that &#8220;the&#8221; indicates the email is dangerous). To minimize such problems, both the training and test sets usually go through a process of <strong>feature selection</strong>, where meaningless information is filtered out so it doesn&#8217;t affect the classifier. If this sounds like more of an art than a science, it is and there are several ways in which errors can manifest:</p>


	<ol>
	<li>Human Bias &#8211; people are necessary to select and classify the training and test sets as well as feature selection. This can create biases in the system that reflect the assumptions of the creators. As Malcolm Gladwell&#8217;s <a href="http://www.newyorker.com/fact/content/articles/060206fa_fact">excellent article on profiling</a> explored, such biases create systems that solve the wrong problems and vulnerabilities exploitable by intelligent attackers.</li>
		<li>Too Little Human Bias &#8211; on the opposite end of the spectrum, it&#8217;s possible to have too much faith in the effectiveness of a classifier. The difficulty here is that the judgement of the computer will be accepted as absolute. One problem is that it generally is impossible to extricate any clear explanation of the classification&#8217;s reasoning (any explanation is like teasing out thought at the neuron level, making it too low-level to be sensible). Furthermore, even if such explanations were available, the experience with the <span class="caps">TSA</span>&#8217;s No Fly Lists suggests they would not be made available to agents acting on the classifier output. At best, this would only mean that the <span class="caps">NSA</span> is inundated with erroneous data. At worst, it could lead to extensive spying, internment, and misguided strategic directions. Best not to contemplate this further.</li>
		<li>False Positives and Negatives &#8211; any classification system usually contains some mix of false positives and negatives. In this system, the political pressures seem to mandate that the false negatives (ie, missing a threat) should be 0 if at all possible. Unfortunately, minimizing the false negatives invariably increases the false positives, meaning that more events will be erroneously triggered. For the dangers this presents, look at item #2.</li>
		<li>The Wrong Tool For the Job &#8211; Even if the classifier were able to achieve a remarkable level of correctness and accuracy, it&#8217;s still possible it would be the wrong tool for the job. As one blog has observed, <a href="http://alexandertheaverage.blogspot.com/2006/02/nsa-scandal-got-you-worried-then-i.html">Osama Bin Ladin probably isnâ€™t clicking around on Amazon</a>, meaning that these tools for signal intelligence won&#8217;t be very useful if the enemy is not creating any signals for them to detect. How likely are terrorist cells going to be using email when you can fedex or hand-deliver documents anywhere around the globe in a few days? How hard is it to exploit steganography or misdirection to thwart any tracking systems? In this case, this elaborate <span class="caps">NSA</span> system just becomes another example of America&#8217;s heavy reliance on technology (smart bombs, sigint, spy satellites) over the crude and dirty human-based methods of gathering information and waging war. How many gaps in our knowledge will this system ever fill? Or are there other ways to more effectively gather information for the cost of building this surveillance infrstructure?</li>
	</ol>


	<p><em>freedom Kennedy chameleon man mindwar <span class="caps">BROMURE </span>Echelon <span class="caps">TELINT </span>Armani Marxist Bletchley Park <span class="caps">FIPS140</span> nuclear supercomputer mania <span class="caps">USDOJ</span></em></p>


	<p>Of course, the <span class="caps">NSA</span> will claim their systems are effective and already performing vital tasks in the war on terror. Indeed, the article about <span class="caps">ADVISE</span> reports &#8220;the system &#8211; parts of which are operational, parts of which are still under development &#8211; is already credited with helping to foil some plots.&#8221; But what are the nature of these plots? Even if I give the <span class="caps">NSA</span> credit and assume they aren&#8217;t being duplicitous about current problems in the hopes they can fix them later (a condition known as <em>hope creep</em>), I wonder if the same circular logic presented in the case of Guantanamo Bay detainees will also be applied here: <a href="http://news.bbc.co.uk/1/hi/world/middle_east/4708946.stm">Guantanamo Bay is only for Al Qaeda terrorists, therefore everybody interred there must be a dangerous terrorist</a>. Again, what are these thwarted attacks? Are they real, orchestrated and viable or just some bored teens talking smack on MySpace? Such information will never be made publicbecause the <span class="caps">NSA</span> needs foiled plots to justify the system and thus any vagule plausible detected messages will become foiled terrorist plots. Meanwhile we lurch closer to national insolvency, confident in our abilities to detect the next 9/11 if only those darn terrorists would play by our expected rules. And we lay the groundwork of a national surveillance state, just ready to be exploited by some avaricious future leaders.</p>


	<p>So, what is to be done? I wish there was something. If the stakes were not so high, I&#8217;d suggest that it might be worthwhile to update M-x spook with a whole new lexicon to sabotage the surveillance mechanisms now before more money is thrown into the whole. But these are no times for pranksters, and the only real solution is for Congress to exert the oversight they have. They killed <span class="caps">TIA</span> one already and the could do it again. The oversight is theirs; if only they had the courage and the wisdom to use it.</p>
